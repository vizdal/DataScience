{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MC3",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OLf1iAnXoLN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "b302ec36-7904-4b18-dc08-910535254d09"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lT6ldpY-oQ7V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUfNO_4TonoI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence,one_hot\n",
        "import codecs\n",
        "\n",
        "embed_index = {}\n",
        "glove_vector_path = 'drive/MC/glove.6B.100d.txt'\n",
        "def get_glove_values():\n",
        "  with codecs.open(glove_vector_path,'rb',encoding='utf-8') as f:\n",
        "      print(\"========> Importing GloVe Vector  <=========\")\n",
        "      #Parsing GloVe dataset\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0].lower()\n",
        "          embed_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "      print(\"Number of vectors loaded : \"+ str(len(embed_index)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6R7fM6bouve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f4f2d468-aafa-49dc-c610-af27032fdf79"
      },
      "cell_type": "code",
      "source": [
        "get_glove_values()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========> Importing GloVe Vector  <=========\n",
            "Number of vectors loaded : 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N352UuWiozry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, merge\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tensor\n",
        "\n",
        "CONST_INPUT_SIZE = 1\n",
        "\n",
        "vocab_context={}\n",
        "vocab_question={}\n",
        "\n",
        "def tokenize_and_embed_words(inputValue,isContext):\n",
        "  words_in_input = text_to_word_sequence(inputValue)\n",
        "  count_words_input = 0\n",
        "  if isContext :\n",
        "    vocab_context = np.zeros((len(words_in_input), 100))\n",
        "    for word in words_in_input:\n",
        "      if word in embed_index.keys():\n",
        "        vocab_context[count_words_input] = embed_index[word]\n",
        "        count_words_input += 1\n",
        "    actual_data = vocab_context[0:count_words_input]\n",
        "  else:\n",
        "    vocab_question = np.zeros((len(words_in_input), 100))\n",
        "    for word in words_in_input:\n",
        "      if word in embed_index.keys():\n",
        "        vocab_question[count_words_input] = embed_index[word]\n",
        "        count_words_input += 1\n",
        "    actual_data = vocab_question[0:count_words_input]\n",
        "  return actual_data \n",
        "    \n",
        "def import_squad_dataset():\n",
        "    #To parse the glove values in to an array form\n",
        "    print(\"Step 1: Started Context Tokenization...\")\n",
        "    squad_data_set='drive/MC/Dev.json'\n",
        "    with codecs.open(squad_data_set,'rb',encoding='utf-8') as data_file:\n",
        "        data = json.load(data_file)\n",
        "        data_entity_list = []\n",
        "        count_iterations = 0\n",
        "        question='';\n",
        "        for item in data['data']:\n",
        "          paragraph = item['paragraphs']\n",
        "          for ctx in paragraph:\n",
        "            if count_iterations < CONST_INPUT_SIZE:\n",
        "               context = ctx['context']\n",
        "               vocab_context = tokenize_and_embed_words(context,True);\n",
        "               print(vocab_context)\n",
        "               qas = ctx['qas']\n",
        "               for question_text in qas:\n",
        "                 question = question_text['question']\n",
        "                 vocab_question = tokenize_and_embed_words(question,False);\n",
        "                 break\n",
        "               # model1\n",
        "               size_context = len(vocab_context)\n",
        "               size_question = len(vocab_question)\n",
        "               \n",
        "               vocab_context_tensor = tensor.convert_to_tensor(vocab_context,dtype=tensor.float32)\n",
        "               embed_context = Embedding(input_dim=size_context, output_dim=100,input_length=size_context,trainable=False)\n",
        "               tensor_context = embed_context(vocab_context_tensor)\n",
        "               lstm_context = Bidirectional(LSTM(100))\n",
        "               lstm_context_out = lstm_context(tensor_context)\n",
        "               print(lstm_context_out)\n",
        "               #drop_1 = Dropout(0.5)(lstm_out)\n",
        "\n",
        "               # model2\n",
        "               vocab_question_tensor = tensor.convert_to_tensor(vocab_question,dtype=tensor.float32)\n",
        "               embed_question = Embedding(input_dim=size_question, output_dim=100,input_length=size_question,trainable=False)\n",
        "               tensor_context = embed_context(vocab_question_tensor)\n",
        "               lstm_question = Bidirectional(LSTM(100))\n",
        "               lstm_question_out = lstm_question(tensor_context)\n",
        "               print(lstm_question_out)\n",
        "                \n",
        "               #merged_tensor = merge([lstm_context_out, lstm_question_out], mode='dot',dot_axes=-1)\n",
        "               #print(merged_tensor.summary())\n",
        "               \n",
        "               #model = Sequential()\n",
        "               #model.add(Merge([lstm_context_out, lstm_question_out], mode='concat'))\n",
        "               #model.add(Dense(1))\n",
        "               #model.add(Activation('sigmoid'))\n",
        "               #model.compile(optimizer='RMSprop', loss='binary_crossentropy')\n",
        "               #model.fit([X[:,:m,:], X[:,m:,:]], y)\n",
        "               #ques_input = Input(shape=(50, ), dtype='int32', name='ques_input')\n",
        "               #x = Embedding(input_dim=vocab_size, output_dim=50, weights=[vocab_question],input_length=50)\n",
        "               #lstm_out = Bidirectional(LSTM(256, return_sequences=True, implementation=2), merge_mode='concat')(x)\n",
        "               #drop_2 = Dropout(0.5)(lstm_out) \n",
        "              \n",
        "              \n",
        "              #words_in_paragraph = tokenize_text(context)\n",
        "               #words_in_paragraph = text_to_word_sequence(context)\n",
        "               #vocab_context = np.zeros((len(words_in_paragraph), 100))\n",
        "               #vocab_question = np.zeros((len(words_in_paragraph), 100\n",
        "               #count_words_question = 0\n",
        "               #for word in words_in_paragraph:\n",
        "                  #if word in embed_index.keys():\n",
        "                 #   vocab_context[count_words_question] = embed_index[word]\n",
        "                  #  count_words_question += 1\n",
        "               #actual_data = vocab_context[0:count_words_question]\n",
        "               #model = Sequential()\n",
        "               #model.add(Embedding(input_dim=count_words_question,output_dim=count_words_question, input_length=100))\n",
        "               #e = Embedding(count_words_question, 100, weights=actual_data)\n",
        "               #model.add(e)\n",
        "               #model=(Bidirectional(LSTM(count_words_question,use_bias=True)))\n",
        "               #model.add(Dense(count_words_question, activation='sigmoid'))\n",
        "               # compile the model\n",
        "               #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "               # summarize the model\n",
        "               #print(model)\n",
        "               #print(actual_data)\n",
        "               count_iterations += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7Qn-Nkxo9YD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "42e39db2-9023-46d9-8eda-65c5f896837c"
      },
      "cell_type": "code",
      "source": [
        "import_squad_dataset()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1: Started Context Tokenization...\n",
            "[[-0.44532999 -0.44969001  1.23930001 ...  0.65855998  0.79881001\n",
            "  -0.16019   ]\n",
            " [-0.80625999  0.36656001  0.46963999 ...  0.91166002  1.12919998\n",
            "   0.71038997]\n",
            " [ 0.070568    0.55312002 -0.055536   ...  0.18748     0.64086002\n",
            "  -0.15872   ]\n",
            " ...\n",
            " [ 0.26333001  0.41064    -0.35883999 ... -1.01520002  0.33160001\n",
            "   0.83611   ]\n",
            " [-0.26717001  0.74791002 -0.11579    ... -0.54696    -0.62404001\n",
            "   0.3809    ]\n",
            " [ 0.070568    0.55312002 -0.055536   ...  0.18748     0.64086002\n",
            "  -0.15872   ]]\n",
            "Tensor(\"bidirectional_17/concat:0\", shape=(123, 200), dtype=float32)\n",
            "Tensor(\"bidirectional_18/concat:0\", shape=(10, 200), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VIFR8Qf4o_W1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "53b0e1e6-9999-4695-e947-17fa3ba076bb"
      },
      "cell_type": "code",
      "source": [
        "print(vocab_context)\n",
        "print(\"Question Vector\")\n",
        "print(vocab_question)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n",
            "Question Vector\n",
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lWB_IXQYuUNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}